{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e6ae50e-220e-4356-9b76-7d4952027535",
   "metadata": {},
   "source": [
    "Project : CamemBERT  \n",
    "Unit : Advanced Machine Learning   \n",
    "MSc. Intelligent systems engineering  \n",
    "SORBONNE UNIVERSITÉ  \n",
    "\n",
    "--- Students ---  \n",
    "@SSivanesan - Shivamshan SIVANESAN  \n",
    "@Emirtas7 - Emir TAS  \n",
    "@KishanthanKingston - Kishanthan KINGSTON  \n",
    "code inspiré de :\n",
    "- https://huggingface.co/blog/how-to-train\n",
    "- https://github.com/theartificialguy/NLP-with-Deep-Learning/blob/master/BERT/Fine%20Tune%20BERT/fine_tuning_bert_with_MLM.ipynb\n",
    "- https://github.com/mdabashar/Deep-Learning-Algorithms/blob/master/Retraining%20RoBERTa%20for%20MLM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6cc68-7f66-4e34-96a9-41958ddfe5dd",
   "metadata": {},
   "source": [
    "#### Roberta pré-entrainé sur une petite part de notre base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4a3a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 13:19:11.021541: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-10 13:19:12.033517: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-10 13:19:12.033612: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-10 13:19:12.033624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaForMaskedLM,AutoTokenizer \n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from utilis.utilis import PreProcessing\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dab691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'fr_part_1_short.txt'\n",
    "pre_process = PreProcessing(file_path)\n",
    "input_texts = pre_process.read_dataset() \n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_len=512)\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0555568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, block_size:int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, truncation=True, max_length=512, padding='max_length') \n",
    "            examples = {\"input_ids\": inputs.input_ids}\n",
    "            grouped_examples = self.group_texts(examples, block_size)\n",
    "            self.inputs.extend(grouped_examples[\"input_ids\"])\n",
    "            self.targets.extend(grouped_examples[\"labels\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"input_ids\": self.inputs[idx], \"labels\": self.targets[idx]}\n",
    "        return item\n",
    "\n",
    "    def group_texts(self,examples, block_size):\n",
    "        concatenated_examples = {k: examples[k] for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "dataset = MyDataset(input_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7bef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta-mlm\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10,\n",
    "    save_total_limit=50,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6f42c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 07:54, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_result = trainer.train()\n",
    "trainer.save_model(\"./roberta-retrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e51115bb-890e-4fd4-9728-f2ae0a886fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.20697994530200958,\n",
       "  'token': 4532,\n",
       "  'token_str': ' maximum',\n",
       "  'sequence': 'Je prépare un gateau au maximum.'},\n",
       " {'score': 0.06615154445171356,\n",
       "  'token': 5059,\n",
       "  'token_str': ' blog',\n",
       "  'sequence': 'Je prépare un gateau au blog.'},\n",
       " {'score': 0.05863066762685776,\n",
       "  'token': 1082,\n",
       "  'token_str': ' site',\n",
       "  'sequence': 'Je prépare un gateau au site.'},\n",
       " {'score': 0.03209709748625755,\n",
       "  'token': 914,\n",
       "  'token_str': ' match',\n",
       "  'sequence': 'Je prépare un gateau au match.'},\n",
       " {'score': 0.027383187785744667,\n",
       "  'token': 11739,\n",
       "  'token_str': ' blend',\n",
       "  'sequence': 'Je prépare un gateau au blend.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./roberta-retrained\",\n",
    "    tokenizer=\"roberta-base\"\n",
    ")\n",
    "fill_mask(\"Je prépare un <mask> au chocolat.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
